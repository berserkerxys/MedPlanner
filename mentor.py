import streamlit as st
import time
import google.generativeai as genai

# Tenta importar Groq de forma segura
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

# --- GERENCIAMENTO DE CHAVES ---
def get_secret(key):
    # Tenta pegar na raiz
    val = st.secrets.get(key, None)
    # Se n√£o achar, procura dentro do bloco supabase (caso o toml esteja mal formatado)
    if not val and "supabase" in st.secrets:
        if isinstance(st.secrets["supabase"], dict):
            val = st.secrets["supabase"].get(key, None)
        elif hasattr(st.secrets["supabase"], key):
            val = getattr(st.secrets["supabase"], key)
    return val

GEMINI_KEY = get_secret("GEMINI_KEY")
GROQ_KEY = get_secret("GROQ_API_KEY")

# --- MOTOR DE INTELIG√äNCIA ---
def configurar_cliente():
    """
    Decide qual IA usar baseada nas chaves dispon√≠veis.
    Prioridade: Groq (Mais r√°pida/Llama 3) > Gemini (Google)
    """
    # 1. Tenta Groq (Llama 3)
    if GROQ_AVAILABLE and GROQ_KEY:
        try:
            client = Groq(api_key=GROQ_KEY)
            # Retorna: (provider_name, client_object, model_name)
            return "groq", client, "llama3-70b-8192"
        except Exception as e:
            print(f"Erro ao iniciar Groq: {e}")
    
    # 2. Tenta Gemini (Google)
    if GEMINI_KEY:
        try:
            genai.configure(api_key=GEMINI_KEY)
            model = genai.GenerativeModel('gemini-2.0-flash')
            return "gemini", model, "gemini-2.0-flash"
        except Exception as e:
            print(f"Erro ao iniciar Gemini: {e}")

    return None, None, None

def render_mentor(conn_ignored):
    st.header("ü§ñ Mentor IA - MedPlanner")
    
    # Inicializa o motor
    provider, client, model_name = configurar_cliente()
    
    # Indicador de Status
    if provider == "groq":
        st.caption(f"üü¢ **Conectado:** Llama 3 (via Groq) | ‚ö° Alta Velocidade")
    elif provider == "gemini":
        st.caption(f"üîµ **Conectado:** Gemini Flash (via Google) | üß† Alta Precis√£o")
    else:
        st.warning("‚ö†Ô∏è Modo Offline (Sem chaves configuradas).")
        st.caption("Adicione `GROQ_API_KEY` ou `GEMINI_KEY` aos segredos.")

    # Hist√≥rico do Chat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
        # Mensagem de Boas-Vindas
        welcome_msg = "Ol√°, Doutor(a)! Sou seu preceptor virtual. Posso criar mnem√¥nicos, explicar fisiopatologia ou discutir casos cl√≠nicos. Qual o foco de hoje?"
        st.session_state.chat_history.append({"role": "assistant", "content": welcome_msg})

    # Renderiza Hist√≥rico
    for msg in st.session_state.chat_history:
        # Normaliza √≠cones
        avatar = "ü§ñ" if msg["role"] == "assistant" else "üë®‚Äç‚öïÔ∏è"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # Input do Usu√°rio
    if prompt := st.chat_input("Ex: Diferen√ßa entre S√≠ndrome Nefr√≠tica e Nefr√≥tica..."):
        
        # 1. Adiciona e exibe pergunta do usu√°rio
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üë®‚Äç‚öïÔ∏è"):
            st.markdown(prompt)

        # 2. Processa resposta da IA
        with st.chat_message("assistant", avatar="ü§ñ"):
            message_placeholder = st.empty()
            full_response = ""
            
            if client:
                try:
                    # --- ESTRAT√âGIA GROQ ---
                    if provider == "groq":
                        # Prepara mensagens no formato OpenAI
                        messages = [
                            {"role": "system", "content": "Voc√™ √© um mentor experiente de resid√™ncia m√©dica no Brasil. Responda de forma did√°tica, direta e focada em provas (R1/R3). Use negrito para conceitos chave e cite guidelines recentes (SBC, SBP, FEBRASGO)."},
                        ]
                        for m in st.session_state.chat_history:
                            # Filtra hist√≥rico para evitar erros de role
                            role = "assistant" if m["role"] == "assistant" else "user"
                            messages.append({"role": role, "content": m["content"]})
                        
                        stream = client.chat.completions.create(
                            model=model_name,
                            messages=messages,
                            stream=True,
                            temperature=0.6 # Equil√≠brio entre criatividade e precis√£o
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                message_placeholder.markdown(full_response + "‚ñå")

                    # --- ESTRAT√âGIA GEMINI ---
                    elif provider == "gemini":
                        # Prepara hist√≥rico no formato Google
                        gemini_history = []
                        for m in st.session_state.chat_history[:-1]: # Ignora a √∫ltima (que √© o prompt atual)
                            role = "model" if m["role"] == "assistant" else "user"
                            gemini_history.append({"role": role, "parts": [m["content"]]})
                        
                        chat = client.start_chat(history=gemini_history)
                        response = chat.send_message(prompt, stream=True)
                        
                        for chunk in response:
                            if chunk.text:
                                full_response += chunk.text
                                message_placeholder.markdown(full_response + "‚ñå")

                    # Finaliza visualiza√ß√£o
                    message_placeholder.markdown(full_response)

                except Exception as e:
                    # Tratamento de Erro Unificado
                    error_msg = str(e).lower()
                    if "429" in error_msg or "quota" in error_msg or "rate limit" in error_msg:
                        full_response = "‚ö†Ô∏è **Mentor Sobrecarregado.**\nAtingimos o limite de velocidade da IA momentaneamente. Aguarde 30 segundos e tente novamente."
                        message_placeholder.warning(full_response)
                    else:
                        full_response = f"Erro t√©cnico na conex√£o: {e}"
                        message_placeholder.error(full_response)
            else:
                # Fallback Offline
                time.sleep(1)
                full_response = "**[Modo Demo]** Configure uma API Key (Groq ou Gemini) para respostas reais.\n\n"
                full_response += f"Sua pergunta sobre *'{prompt}'* √© relevante. Foque nos crit√©rios diagn√≥sticos e tratamento inicial."
                message_placeholder.markdown(full_response)

        # 3. Salva no hist√≥rico se n√£o for erro
        if full_response and "Erro t√©cnico" not in full_response:
            st.session_state.chat_history.append({"role": "assistant", "content": full_response})