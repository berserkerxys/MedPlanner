import streamlit as st
import time
import google.generativeai as genai

# Tenta importar Groq de forma segura
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

# --- GERENCIAMENTO DE CHAVES ---
def get_secret(key):
    val = st.secrets.get(key, None)
    if not val and "supabase" in st.secrets:
        if isinstance(st.secrets["supabase"], dict):
            val = st.secrets["supabase"].get(key, None)
        elif hasattr(st.secrets["supabase"], key):
            val = getattr(st.secrets["supabase"], key)
    return val

GEMINI_KEY = get_secret("GEMINI_KEY")
GROQ_KEY = get_secret("GROQ_API_KEY")

# --- MOTOR DE INTELIG√äNCIA ---
def configurar_cliente():
    """
    Decide qual IA usar.
    Prioridade: Groq (Llama 3.3) > Gemini 2.0 Flash
    """
    # 1. Tenta Groq (Modelo Atualizado: llama-3.3-70b-versatile)
    if GROQ_AVAILABLE and GROQ_KEY:
        try:
            client = Groq(api_key=GROQ_KEY)
            # Modelo mais recente e est√°vel da Groq (Fev/2026)
            return "groq", client, "llama-3.3-70b-versatile"
        except Exception as e:
            print(f"Erro ao iniciar Groq: {e}")
    
    # 2. Tenta Gemini (Google)
    if GEMINI_KEY:
        try:
            genai.configure(api_key=GEMINI_KEY)
            model = genai.GenerativeModel('gemini-2.0-flash')
            return "gemini", model, "gemini-2.0-flash"
        except Exception as e:
            print(f"Erro ao iniciar Gemini: {e}")

    return None, None, None

def render_mentor(conn_ignored):
    st.header("ü§ñ Mentor IA - MedPlanner")
    
    provider, client, model_name = configurar_cliente()
    
    # Status
    if provider == "groq":
        st.caption(f"üü¢ **Conectado:** Llama 3.3 (Groq) | ‚ö° Ultra R√°pido")
    elif provider == "gemini":
        st.caption(f"üîµ **Conectado:** Gemini 2.0 (Google) | üß† Alta Precis√£o")
    else:
        st.warning("‚ö†Ô∏è Modo Offline (Sem chaves configuradas).")
        st.caption("Adicione `GROQ_API_KEY` ou `GEMINI_KEY` aos segredos.")

    # Hist√≥rico
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
        welcome_msg = "Ol√°, Doutor(a)! Sou seu preceptor virtual. Posso criar mnem√¥nicos, explicar fisiopatologia ou discutir casos cl√≠nicos. Qual o foco de hoje?"
        st.session_state.chat_history.append({"role": "assistant", "content": welcome_msg})

    for msg in st.session_state.chat_history:
        avatar = "ü§ñ" if msg["role"] == "assistant" else "üë®‚Äç‚öïÔ∏è"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # Input
    if prompt := st.chat_input("Ex: Diferen√ßa entre S√≠ndrome Nefr√≠tica e Nefr√≥tica..."):
        
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üë®‚Äç‚öïÔ∏è"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar="ü§ñ"):
            message_placeholder = st.empty()
            full_response = ""
            
            if client:
                try:
                    # --- L√ìGICA GROQ (Llama 3.3) ---
                    if provider == "groq":
                        messages = [
                            {"role": "system", "content": "Voc√™ √© um mentor experiente de resid√™ncia m√©dica no Brasil. Responda de forma did√°tica, direta e focada em provas (R1/R3). Use negrito para conceitos chave e cite guidelines recentes (SBC, SBP, FEBRASGO)."},
                        ]
                        for m in st.session_state.chat_history:
                            role = "assistant" if m["role"] == "assistant" else "user"
                            messages.append({"role": role, "content": m["content"]})
                        
                        stream = client.chat.completions.create(
                            model=model_name,
                            messages=messages,
                            stream=True,
                            temperature=0.6
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                message_placeholder.markdown(full_response + "‚ñå")

                    # --- L√ìGICA GEMINI ---
                    elif provider == "gemini":
                        gemini_history = []
                        for m in st.session_state.chat_history[:-1]:
                            role = "model" if m["role"] == "assistant" else "user"
                            gemini_history.append({"role": role, "parts": [m["content"]]})
                        
                        chat = client.start_chat(history=gemini_history)
                        response = chat.send_message(prompt, stream=True)
                        
                        for chunk in response:
                            if chunk.text:
                                full_response += chunk.text
                                message_placeholder.markdown(full_response + "‚ñå")

                    message_placeholder.markdown(full_response)

                except Exception as e:
                    # Tratamento de Erro Robusto
                    error_msg = str(e).lower()
                    
                    # Erro de Cota ou Modelo Inv√°lido (como o 400 que voc√™ recebeu)
                    if "400" in error_msg or "429" in error_msg or "model" in error_msg:
                        
                        # Tenta Fallback IMEDIATO para o Gemini se a Groq falhou
                        if provider == "groq" and GEMINI_KEY:
                            try:
                                genai.configure(api_key=GEMINI_KEY)
                                model_gemini = genai.GenerativeModel('gemini-2.0-flash')
                                # Recria contexto para Gemini
                                g_hist = []
                                for m in st.session_state.chat_history[:-1]:
                                    r = "model" if m["role"] == "assistant" else "user"
                                    g_hist.append({"role": r, "parts": [m["content"]]})
                                
                                chat = model_gemini.start_chat(history=g_hist)
                                res = chat.send_message(prompt)
                                full_response = f"*(Fallback para Gemini)*\n\n{res.text}"
                                message_placeholder.markdown(full_response)
                            except:
                                full_response = "‚ö†Ô∏è **Erro nos Provedores de IA.**\nAmbos os modelos (Groq e Gemini) est√£o indispon√≠veis no momento. Tente mais tarde."
                                message_placeholder.error(full_response)
                        else:
                            full_response = f"‚ö†Ô∏è **Erro na IA ({provider}):**\n{e}\n\nTente recarregar a p√°gina ou verifique as chaves."
                            message_placeholder.error(full_response)
                    else:
                        full_response = f"Erro t√©cnico: {e}"
                        message_placeholder.error(full_response)
            else:
                time.sleep(1)
                full_response = "**[Modo Demo]** Configure uma API Key para respostas reais."
                message_placeholder.markdown(full_response)

        if full_response and "Erro t√©cnico" not in full_response:
            st.session_state.chat_history.append({"role": "assistant", "content": full_response})