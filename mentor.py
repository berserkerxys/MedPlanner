import streamlit as st
import time
import google.generativeai as genai

# Tenta carregar a chave dos segredos
API_KEY = st.secrets.get("GEMINI_KEY", None)

def configurar_ia():
    if API_KEY:
        genai.configure(api_key=API_KEY)
        # Configura o modelo
        model = genai.GenerativeModel(
            model_name='gemini-2.0-flash', # Modelo r√°pido e inteligente
            system_instruction="""
            Voc√™ √© um Mentor de Resid√™ncia M√©dica experiente e did√°tico.
            Seu objetivo √© ajudar estudantes de medicina e m√©dicos rec√©m-formados.
            
            Diretrizes:
            1. Seja direto e focado em provas de resid√™ncia (R1).
            2. Use termos t√©cnicos corretos, mas explique de forma simples.
            3. Sempre que poss√≠vel, forne√ßa mnem√¥nicos para memoriza√ß√£o.
            4. Se a pergunta for sobre conduta, cite os guidelines mais recentes (ex: AHA, ADA, MS-BR).
            5. N√£o d√™ diagn√≥sticos para casos reais de pacientes (aviso legal). Foco acad√™mico.
            """
        )
        return model
    return None

def render_mentor(conn_ignored):
    st.header("ü§ñ Mentor IA - MedPlanner")
    st.caption("Seu assistente cl√≠nico 24h para tirar d√∫vidas e revisar conceitos.")

    # Verifica conex√£o
    if API_KEY:
        model = configurar_ia()
        st.success("üü¢ Conectado ao Google Gemini")
    else:
        st.warning("‚ö†Ô∏è Modo Demonstra√ß√£o (Sem API Key). Adicione 'GEMINI_KEY' ao secrets.toml.")
        model = None

    # Hist√≥rico do Chat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
        # Mensagem inicial
        st.session_state.chat_history.append({
            "role": "assistant", 
            "content": "Ol√°, Doutor(a)! Qual tema vamos dominar hoje? Posso explicar fisiopatologia, criar mnem√¥nicos ou discutir quest√µes."
        })

    # Renderiza mensagens anteriores
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Input do Usu√°rio
    if prompt := st.chat_input("Ex: 'Qual a tr√≠ade de Cushing?' ou 'Mnem√¥nico para causas de Pancreatite'"):
        
        # 1. Exibe e salva pergunta
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. Gera resposta
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            if model:
                try:
                    # Envia para o Gemini com hist√≥rico (contexto)
                    # O Gemini espera hist√≥rico no formato: [{'role': 'user'/'model', 'parts': ['text']}]
                    history_gemini = [
                        {"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]}
                        for m in st.session_state.chat_history if m["role"] != "system"
                    ]
                    
                    chat = model.start_chat(history=history_gemini[:-1])
                    response = chat.send_message(prompt, stream=True)
                    
                    for chunk in response:
                        if chunk.text:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "‚ñå")
                    
                    message_placeholder.markdown(full_response)
                    
                except Exception as e:
                    st.error(f"Erro na IA: {e}")
                    full_response = "Tive um problema de conex√£o. Tente novamente em instantes."
            else:
                # Fallback Demo
                time.sleep(1)
                full_response = "**[Modo Demo]** Resposta simulada.\n\n"
                full_response += f"Sobre *{prompt}*, o conceito chave para provas √© focar na apresenta√ß√£o cl√≠nica t√≠pica."
                message_placeholder.markdown(full_response)

        # 3. Salva resposta
        st.session_state.chat_history.append({"role": "assistant", "content": full_response})